---
title: "PART -3 Level up: NLP & emotion detection"
author: "Daniel Wang"
date: "6/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 10, fig.height = 7.5)
```

```{css, echo= FALSE}
body {
  font-family: Helvetica;
  padding: 20px;
  background: #ffffff;
}

.header {
  padding: 30px;
  font-size: 40px;
  text-align: center;
  background: white;
}

.row:after {
  content: "";
  display: table;
  clear: both;
}

.footer {
  padding: 20px;
  text-align: center;
  background: #ddd;
  margin-top: 20px;
}

```

## Introduction

In the last few projects, I used SmappR, %in% and ggplot to create reports based on word frequencies. We knew that in this database, the most popular ingredients are amber, musk, vanilla, sandalwood. Later on, I was trying to figure out how each notes are "paired" by using Grep. For example, I can use Grep to search all formulas that contain the ingredients "bergamot", "jasmine" and "rose". 

However, this data base has a richer corpus that are waiting to be explored, the "description" data base. Let's first have a look on what a "description" in this data base looks alike: 
```{R, echo = FALSE, message = FALSE, warning = FALSE, results = FALSE}
install.packages("devtools")
library(devtools)
library(tidyverse)
install_github('haozhu233/kableExtra')
library(kableExtra)
install.packages("topicmodels")
library(topicmodels)
library(tm)
library(formattable)
library(tidytext)
library(tidyr)
install_github('SMAPPNYU/smappR')
library(smappR)
install.packages("ggrepel")
library(ggrepel)
install.packages("qdap")
library(qdap)
library(remotes)
install_github("EmilHvitfeldt/textdata")
library(textdata)
read.csv("https://www.dropbox.com/s/wvzevr0z39uf44x/final_perfume_data.csv?dl=1") -> perfume_data
perfume_data %>%
  mutate_all(~ifelse(. %in% c("null", ""), NA, .)) %>%
  na.omit() -> perfume_data_clean
```


```{R, echo = FALSE, message = FALSE, warning = FALSE, results = TRUE}
slice(perfume_data_clean, 835) %>%
  kbl() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "500px")
```
The "description" contains at least 4 sets of important information: 

1. The key ingredients composition. Key ingredients are a set of ingredients that makes the "note" of a perfume. This is exactly what we would like to gather from "make a palette" project. In this example, the wood, sage, rubbery black tea, cacao, and styrax are the key ingredients in this perfum.  

2. The "notes" (namely the theme & topic) of a perfume. "Notes" are a way to address a sort of perfume which holds similar "key ingredients". Just like we can categorize different articles into romantics, Si-Fi, or thriller, we can also categorize perfumes into white flower notes or citrus notes. In this example, **Woody Mood Eau de Parfum** is a **Wood Notes Perfume**. You can explore more in https://www.fragrantica.com/notes/! 

3. The projection of perfume. "Projection" here means what the impression it leaves on people. For example, what images, memories, or sensations it can evoke. In our example, it is **fantasies of fall â€“ crisp air, sweet wood smoke, the crackle of leaves hitting the bonfire, compiling them into an autumnal essay that satisfies down to the bone.**  

4. The emotional value of perfume. Look at the following words: **beautifully**, **radiant**, **perfect**, **satisfactory**, **no-brainier**. It seems the reviewer is quite happy with this perfume. 

So that is what we are going to explore today. We are going to use tidy text to convert our descriptions into tidy format, use NRC emotional lexicon to map out the emotional demographic of the database, and pull out "projections" related to a certain "note", using one of NLP's method:  Latent Dirichlet Allocation (LDA).  



## Step 1: Prepare the data


```{R, echo = TRUE, message = FALSE, warning = FALSE, results = FALSE}
des <- perfume_data_clean$Description
des_df <- tibble(line= 1:2111, text = des)
des_df_raw <- des_df %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)
  rm(des)
  rm(des_df)
```
The **des_df_raw** data frame contains the so called " tidy text format". (Slige and Robinson, 2017) Each description was broken into a 2-column table. The first column is the position where this description hold (an identifier), the second column is the description broken into each individual piece. For detailed explanation, please refer to https://www.tidytextmining.com/tidytext.html. Please note I used anti_join here to filter out the stop words (i,e. this, that, is, an, etc.) Let's slice part of it to see what it looks like: 

```{R, echo = FALSE, message = FALSE, warning = FALSE, results = TRUE}
slice(des_df_raw, 2335:2347) %>%
  kbl() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "500px")
```


## Step 2: Notes Popularity

When descriptions are morphed into tidy text format, we can easily count word frequencies using **count()** command. 

```{R, echo = TRUE, message = FALSE, warning = FALSE, results = TRUE}
des <- perfume_data_clean$Description
des_df <- tibble(line= 1:2111, text = des)
des_df_duo <- des_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE) 
rm(des)
rm(des_df)

notes_trend <- des_df_duo %>%
  filter(!word1 %in% c("top", "middle", "base"), 
         word2 == "notes") %>%
  slice(1:50) 
  ggplot(notes_trend, aes(x = reorder (word1, n), y = n)) +
  geom_col() +
  labs(title="Notes Popularity",
       x = "Notes",
       y = "Frequency") +
  coord_flip()
```

Some technical explanation: 

 1. ngrams is used to see how often word X is followed by word Y. For example, "plead" might often followed by "guilty" in a court's trail transcript . Here we want to see what words follows after the word "notes". So we first list all the possible pairs in the data frame **des_duo**, and then filter all the possible two-word combination with the second word being **notes**.
  
 2. I filtered out the combination "top", "middle", and "base" with "notes".
 

From the graph above, you could see the frequency distribution of notes combination, namely, different "themes" of the perfume. Most of the perfume has more than one notes, the most dominant ones (usually the first & the second notes) are often called "main accords".See the following example (source: fragrantica.com): 

![A Dior Perfume](F:\R Projects\Perfume\NLP\Dior.JPG)

We can also see that citrus, woody, floral, and gourmand notes are fairly popular in the market. This steps helps us to decide how many themes we will use for the next step. 


## Step 3:  Topic Modeling using LDA

This part is heavily relied on Debbie Lieske's ***Machine Learning and NLP using R: Topic Modeling and Music Classification***, url: https://www.datacamp.com/community/tutorials/ML-NLP-lyric-analysis


First, what is LDA (Latent Dirichlet Allocation)? Basically you could think of that as iterative optimization of document-topic-word match. LDA will first randomly group **documents** as **topics**, and mash up all **words** to randomly fit them into **topics**, and compare it against the original **documents** to optimize the iteration process. It seems a total chaos to begin with and there seems to be endless combinations- but with iterative optimization, it is reachable. Although all far from original documents, the one with higher resemblance wins. 

For a more technical explanation of LDA, refer to http://ethen8181.github.io/machine-learning/clustering_old/topic_model/LDA.html

First of all let's work on the style of the chart a bit
Disclaimer: The styling part is completely derived from Debbie Liske's [article](https://www.datacamp.com/community/tutorials/ML-NLP-lyric-analysis), I will study styling with function a lot more in depth in the future but this time I will just take it from where it was. 

```{R, echo = FALSE, message = FALSE, warning = FALSE, results = FALSE}
my_colors <- c("#E69F00", "#A2CFE8", "#8FE3CC", "#CC79A7", "#D55E00", "#D5000, #E391E6","#EFF5BF" )

theme_lyrics <- function(aticks = element_blank(),
                         pgminor = element_blank(),
                         lt = element_blank(),
                         lp = "none")
{
  theme(plot.title = element_text(hjust = 0.5), 
        axis.ticks = aticks, 
        panel.grid.minor = pgminor, 
        legend.title = lt, 
        legend.position = lp)
}

my_kable_styling <- function(dat, caption) {
  kable(dat, "html", escape = FALSE, caption = caption) %>%
  kable_styling(bootstrap_options = c("striped", "condensed", "bordered"),
                full_width = FALSE)
}

word_chart <- function(data, input, title) {
  data %>%
  ggplot(aes(as.factor(row), 1, label = input, fill = factor(topic) )) +
  geom_point(color = "transparent") +
  geom_label_repel(nudge_x = .2,  
                   direction = "y",
                   box.padding = 0.1,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~topic) +
  theme_lyrics() +
  theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
        panel.grid = element_blank(), panel.background = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        strip.text.x = element_text(size = 11)) +
  labs(x = NULL, y = NULL, title = title) +
  coord_flip()
}
```
Secondly, let's transform our tidy data into document-term matrix. 
```{R, echo = TRUE, message = FALSE, warning = FALSE, results = FALSE}
dtm <- des_df_raw %>%
   count(line, word, sort = TRUE) %>%
   ungroup() %>%
   cast_dtm(line, word, n)

k <- 8 
seed = 1234
lda <- LDA(dtm, k = k, method = "GIBBS", control = list(seed = seed))
```
Here I chose 8 topics, namely 8 notes. I include all notes with frequency higher than 10 from the previous chapter. 

And finally, the output
```{R, echo = TRUE, message = FALSE, warning = FALSE, results = TRUE}
num_words <- 20 

top_terms_per_topic <- function(lda_model, num_words) {
topics_tidy <- tidy(lda_model, matrix = "beta")
top_terms <- topics_tidy %>%
    group_by(topic) %>%
    arrange(topic, desc(beta)) %>%
    slice(seq_len(num_words)) %>%
    arrange(topic, beta) %>%
    mutate(row = row_number()) %>%
    ungroup() %>%
    mutate(topic = paste("Topic", topic, sep = " "))
  title <- paste("Top terms for", k, "Notes")
  word_chart(top_terms, top_terms$term, title)
}
top_terms_per_topic(lda, num_words)
```

In this analysis we not only found a series of projections under each notes: we can summarize them as follows:
```{R, echo = FALSE, message = FALSE, warning = FALSE, results = TRUE}
Notes <- c("Vetiver & Sandalwood",	"Oud & Oriental",	"Vanilla & Gourmand",	"Spicy & Woody", 	"Rose & White Flowers",	"Earthy & Dry",	"Iris & Musk",	"Citrus & Sea")
Projections <- c ("Classical and elegant",	"Exotic, mysterious and dark",	"Sweet, warm and delicous",	"Mordern, unique, sexy and complex",	"Beautiful, gentle, romantic, feminine, and light",	"Aromatic and natural", "Warm, delicate, sensual and feminine",	"Summer, bright, fresh, and cool")

table_notes <- data.frame(Notes, Projections)

table_notes %>%
  kbl() %>%
  kable_styling() 
```
Next time when you wants to choose a perfume but don't know where to start, try to use this table! It all depends on what mood and occasion you are in right now! For example, oriental scents might be perfect for a date night, while citrus scents might suits a summer voyage! And you probably wouldn't want to wear oud oriental scents to the office: it's too heavy!


## Step 4: Emotional value detection 

Most of the perfume would **LOVE** to give to a positive emotional projection (because they want you to buy it!), but emotions are something that are very personal and the same bottle of perfume might elicit completely different emotions on different people! While at the same time, some perfume houses walked another way around, such as ***Serge Lutens***, they literally named one of their perfumes **"The orphan"**, and it smells like **"you are in a depressing purgatory"** (Source: https://www.fragrantica.com/perfume/Serge-Lutens/L-orpheline-26214.html).  

So I would like to do an emotional value detection on our data base to see what it looks like! 

I first would like to filter out all the adjectives and adverbs in the text chunk: 
```{R, echo = TRUE, message = FALSE, warning = FALSE, results = TRUE}
adj_adv_depo <- des_df_raw %>%
    left_join(parts_of_speech) %>%
    filter(pos %in% c("Adjective","Adverb")) %>%
    pull(word) %>%
    unique
```
What I did above was pulling an adjective and adverb depository. I achieved that by the "parts_of_speech" function from tidytext package. "parts_of_speech" is a data frame with 205,895 words and their part of speech.I left joined it so that every word from des_df_raw will be included.Then I specify that I'd like to pull adjective and adverb out. Let's have a look on what the final results look like: 
```{R, echo = FALSE, message = FALSE, warning = FALSE, results = TRUE}
head(adj_adv_depo)
```
Here a used NRC word-emotion association lexicon, it is a dictionary of emotions. For more info, visit https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm
Similar to what I did on the part_of_speech, I inner-joined the adjectives & adverbs with NRC word-emotion association lexicon. Then categorized these words by emotions(positive & negative), and setiments (8 sentiments). 
Let's take a look on the result: 
```{R, echo = TRUE, message = FALSE, warning = FALSE, results = TRUE}
adj_adv_depo_df <- data.frame(adj_adv_depo)
colnames(adj_adv_depo_df) <- c("word") 
nrc_dict <- get_sentiments("nrc")
emo_dict <- inner_join (adj_adv_depo_df, nrc_dict, by = "word") %>%
  filter(sentiment %in% c("anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", "trust")) %>% 
  count(sentiment)
emo_dict %>%
  kbl() %>%
  kable_styling() 

sdi_dict <- inner_join (adj_adv_depo_df, nrc_dict, by = "word") %>%
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)
sdi_dict %>%
  kbl() %>%
  kable_styling() 
```
So here are the outputs! Some fits into my initial hypothesis, some not! It is expected that the positive emotions and trust sentiments are the highest, but negative emotions and other sentiments can not be ignored! In the follow-up research, I'll get my hands more on emtional analysis and image learning! 

See you next time! 

